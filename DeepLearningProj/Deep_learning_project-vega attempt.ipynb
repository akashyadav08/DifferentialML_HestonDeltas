{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Assignment\n",
        "### Elain Balderas and Nina McClure"
      ],
      "metadata": {
        "id": "Qj5sIkNUsQWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set up**"
      ],
      "metadata": {
        "id": "C0AYwNI2tHaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### import libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import math\n",
        "from tqdm import tqdm_notebook\n",
        "import random\n",
        "\n",
        "# representation of real numbers in TF, change here for 32/64 bits\n",
        "real_type = tf.float32\n",
        "# real_type = tf.float64"
      ],
      "metadata": {
        "id": "1tpjH45FscRC"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Black Scholes**"
      ],
      "metadata": {
        "id": "pBSrELI9y5ft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assume the risk-free rate is zero for simplicity, which leads to simplified formulas under Black Scholes. ***(or we can add it in????)***\n",
        "\n",
        "Option value/price:\n",
        "$$ C_t = S_t*N(d_1) - K*N(d_2)$$,\n",
        "\n",
        "where $N()$ denotes the standard normal CDF and\n",
        "\n",
        "$$ d_1 = (log(\\frac{S_t}{K}) + 0.5*\\sigma^2 * T) /\\sqrt{T}$$,\n",
        "$$ d_2 = d_1 - \\sigma*\\sqrt{T} $$\n",
        "\n",
        "We assume the underlying assets follow a random walk, i.e., the underlying asset prices are log-normally distributed and follow a geometric Brownian motion:\n",
        "\n",
        "$$ S_t = S_{t-1}\\exp((- 0.5\\sigma^2)t + \\sigma\\sqrt{t}Z) $$,\n",
        "\n",
        "where $Z$ is a standard normal random variable."
      ],
      "metadata": {
        "id": "zQZ1RiJZzCrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Greeks:\n",
        "\n",
        "$$\n",
        "Delta := \\frac{\\partial C}{\\partial S_t} = N(d_1)\n",
        "$$\n",
        "$$\n",
        "Vega := \\frac{\\partial C}{\\partial \\sigma} = \\frac{1}{\\sqrt{2\\pi}}e^{-0.5*d_1^2} S_t\\sqrt{T - t} \n",
        "$$\n",
        "$$\n",
        "Gamma := \\frac{\\partial^2 C}{\\partial S_t^2} = \\frac{1}{\\sqrt{2\\pi}}e^{-0.5*d_1^2}\\frac{1}{S_t \\sigma \\sqrt{T - t}}\n",
        "$$"
      ],
      "metadata": {
        "id": "AL0Zx0qxm-X6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Train and test data simulation***"
      ],
      "metadata": {
        "id": "hFx88zP-8OIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define prices and greeks  \n",
        "def bsPrice(spot, strike, vol, T):\n",
        "    d1 = (np.log(spot/strike) + 0.5 * vol * vol * T) / vol / np.sqrt(T)\n",
        "    d2 = d1 - vol * np.sqrt(T)\n",
        "    return spot * norm.cdf(d1) - strike * norm.cdf(d2)\n",
        "\n",
        "def bsDelta(spot, strike, vol, T):\n",
        "    d1 = (np.log(spot/strike) + 0.5 * vol * vol * T) / vol / np.sqrt(T)\n",
        "    return norm.cdf(d1)\n",
        "\n",
        "def bsVega(spot, strike, vol, T):\n",
        "    d1 = (np.log(spot/strike) + 0.5 * vol * vol * T) / vol / np.sqrt(T)\n",
        "    return spot * np.sqrt(T) * norm.pdf(d1)\n",
        "\n",
        "def bsGamma(spot, strike, vol, T):\n",
        "    d1 = (np.log(spot/strike) + 0.5 * vol * vol * T) / vol / np.sqrt(T)\n",
        "    return norm.pdf(d1) * (1 / (spot * vol * np.sqrt(T)))\n",
        "\n",
        "# main class\n",
        "class BlackScholes:\n",
        "    \n",
        "    def __init__(self, \n",
        "                 vol=0.2,\n",
        "                 T1=1, \n",
        "                 T2=2, \n",
        "                 K= 1.1,\n",
        "                 volMult=1.5):\n",
        "        \n",
        "        self.spot = 1\n",
        "        self.vol = vol\n",
        "        self.T1 = T1\n",
        "        self.T2 = T2\n",
        "        self.K = K\n",
        "        self.volMult = volMult\n",
        "                        \n",
        "    # training set: returns S1 (mx1), C2 (mx1) and dC2/dS1 (mx1)\n",
        "    def trainingSet(self, m, anti=True, seed=None):\n",
        "    \n",
        "        np.random.seed(seed)\n",
        "        \n",
        "        # 2 sets of normal returns\n",
        "        returns = np.random.normal(size=[m, 2]) # Draws from standard normal\n",
        "        \n",
        "        # SDE\n",
        "        vol0 = self.vol * self.volMult\n",
        "        R1 = np.exp(-0.5*vol0*vol0*self.T1 + vol0*np.sqrt(self.T1)*returns[:,0]) # asset price return period 1\n",
        "        R2 = np.exp(-0.5*self.vol*self.vol*(self.T2-self.T1) + self.vol*np.sqrt(self.T2-self.T1)*returns[:,1]) # asset price return period 2 with different volatility\n",
        "        S1 = self.spot * R1 # spot * return gives asset price in period 1\n",
        "        S2 = S1 * R2 # gives asset price in period 2\n",
        "\n",
        "        # payoff\n",
        "        pay = np.maximum(0, S2 - self.K) # compute payoff \n",
        "        \n",
        "        X = S1.reshape((-1,1)) # this is our input (asset price) \n",
        "        #sigma = (np.ones(m)*self.vol).reshape((-1,1))\n",
        "        #X = np.concatenate((A, sigma), axis = 1)\n",
        "        Y = pay # this is our output (the payoff)\n",
        "            \n",
        "        # differentials - note that we are just looking at one time period\n",
        "        Delta =  np.where(S2 > self.K, R2, 0.0).reshape((-1,1)) # this is  delta  - the differential is zero is price less than strike\n",
        "        dvol = S1*np.exp(-0.5 * self.vol**2)*self.vol + returns[:,1] # dy/dvol\n",
        "        Vega = np.where(S2 > self.K, dvol, 0.0).reshape((-1,1)) # vega\n",
        "        Z = np.concatenate((Delta, Vega), axis = 1)\n",
        "\n",
        "        # sizes X = m*1, Y = m*1, Z = m*2\n",
        "        return X.reshape([-1,1]), Y.reshape([-1,1]), Z#Z.reshape([-1,1]) \n",
        "    \n",
        "    # test set: returns a grid of uniform spots \n",
        "    # with corresponding ground true prices, deltas and vegas\n",
        "    def testSet(self, lower=0.35, upper=1.65, num=1000, seed=None):\n",
        "        \n",
        "        spots = np.linspace(lower, upper, num).reshape((-1, 1))\n",
        "        # compute prices, deltas and vegas\n",
        "        prices = bsPrice(spots, self.K, self.vol, self.T2 - self.T1)\n",
        "        d = bsDelta(spots, self.K, self.vol, self.T2 - self.T1)\n",
        "        vegas = bsVega(spots, self.K, self.vol, self.T2 - self.T1)\n",
        "        #gammas = bsGamma(spots, self.K, self.vol, self.T2 - self.T1)\n",
        "        deltas = np.concatenate((d, vegas), axis = 1)\n",
        "        return spots, spots, prices.reshape((-1, 1)) , deltas, vegas.reshape((-1, 1))#.reshape((-1, 1)) \n",
        "   "
      ],
      "metadata": {
        "id": "R_8GZBgXy4Wn"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = BlackScholes()\n",
        "#X, Y, Z = c.trainingSet(m = 3)\n",
        "#c.trainingSet(m = 3)\n",
        "spots, spots, prices, h, vegas = c.testSet()\n",
        "\n",
        "h.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leT8lEASycrS",
        "outputId": "a8397bf3-2361-45ab-b8c4-462ed763c54d"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feedforward network**"
      ],
      "metadata": {
        "id": "aQ7nQtPttPfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation functions are softplus.\n",
        "\n",
        "$$ softplus(x) = log( 1+ exp(x) ) $$ \n",
        "\n",
        "\n",
        "The derivative of the softplus is \n",
        "\n",
        "$$ f'(x)= exp(x) / ( 1+ exp⁡(x) ) = 1 / ( 1 + exp(−x)),$$\n",
        "\n",
        " which is also called the logistic function."
      ],
      "metadata": {
        "id": "0QE7gaRm9AYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vanilla_net(\n",
        "    input_dim,      # dimension of inputs, e.g. 10\n",
        "    hidden_units,   # units in hidden layers, assumed constant, e.g. 20\n",
        "    hidden_layers,  # number of hidden layers, e.g. 4\n",
        "    seed):          # seed for initialization or None for random\n",
        "    \n",
        "    # set seed\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "    # input layer (this will be the prices and differentials)\n",
        "    xs = tf.compat.v1.placeholder(shape=[None, input_dim], dtype=real_type)\n",
        "    \n",
        "    # connection weights and biases of hidden layers\n",
        "    ws = [None]\n",
        "    bs = [None]\n",
        "    # layer 0 (input) has no parameters\n",
        "    \n",
        "    # layer 0 = input layer\n",
        "    zs = [xs] # eq.3, l=0\n",
        "    \n",
        "    # first hidden layer (index 1)\n",
        "    # weight matrix\n",
        "    ws.append(tf.compat.v1.get_variable(\"w1\", [input_dim, hidden_units], \\\n",
        "        initializer = tf.keras.initializers.variance_scaling(), dtype=real_type))\n",
        "    # bias vector\n",
        "    bs.append(tf.compat.v1.get_variable(\"b1\", [hidden_units], \\\n",
        "        initializer = tf.zeros_initializer(), dtype=real_type))\n",
        "    # graph\n",
        "    zs.append(zs[0] @ ws[1] + bs[1]) # eq. 3, l=1\n",
        "    \n",
        "    # second hidden layer (index 2) to last (index hidden_layers)\n",
        "    for l in range(1, hidden_layers): \n",
        "        ws.append(tf.compat.v1.get_variable(\"w%d\"%(l+1), [hidden_units, hidden_units], \\\n",
        "            initializer = tf.keras.initializers.variance_scaling(), dtype=real_type))\n",
        "        bs.append(tf.compat.v1.get_variable(\"b%d\"%(l+1), [hidden_units], \\\n",
        "            initializer = tf.zeros_initializer(), dtype=real_type))\n",
        "        zs.append(tf.nn.softplus(zs[l]) @ ws[l+1] + bs[l+1]) # eq. 3, l=2..L-1\n",
        "\n",
        "    # output layer (index hidden_layers+1)\n",
        "    ws.append(tf.compat.v1.get_variable(\"w\"+str(hidden_layers+1), [hidden_units, 1], \\\n",
        "            initializer = tf.keras.initializers.variance_scaling(), dtype=real_type))\n",
        "    bs.append(tf.compat.v1.get_variable(\"b\"+str(hidden_layers+1), [1], \\\n",
        "        initializer = tf.zeros_initializer(), dtype=real_type))\n",
        "    # eq. 3, l=L\n",
        "    zs.append(tf.nn.softplus(zs[hidden_layers]) @ ws[hidden_layers+1] + bs[hidden_layers+1]) \n",
        "    \n",
        "    # result = output layer\n",
        "    ys = zs[hidden_layers+1]\n",
        "    \n",
        "    # return input layer, (parameters = weight matrices and bias vectors), \n",
        "    # [all layers] and output layer\n",
        "    return xs, (ws, bs), zs, ys"
      ],
      "metadata": {
        "id": "5cT2SQlmsmFc"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Back-prop and twin net**"
      ],
      "metadata": {
        "id": "zr8ZgvqgtyNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute d_output/d_inputs by (explicit) backprop in vanilla net\n",
        "def backprop(\n",
        "    weights_and_biases, # 2nd output from vanilla_net() \n",
        "    zs):                # 3rd output from vanilla_net()\n",
        "    \n",
        "    ws, bs = weights_and_biases\n",
        "    L = len(zs) - 1\n",
        "    \n",
        "    # backpropagation, eq. 4, l=L..1\n",
        "    zbar = tf.ones_like(zs[L]) # zbar_L = 1\n",
        "    for l in range(L-1, 0, -1):\n",
        "        zbar = (zbar @ tf.transpose(ws[l+1])) * tf.nn.sigmoid(zs[l]) # eq. 4\n",
        "    # for l=0\n",
        "    zbar = zbar @ tf.transpose(ws[1]) # eq. 4\n",
        "    \n",
        "    xbar = zbar # xbar = zbar_0\n",
        "    \n",
        "    # dz[L] / dx\n",
        "    return xbar    \n",
        "\n",
        "# combined graph for valuation and differentiation\n",
        "def twin_net(input_dim, hidden_units, hidden_layers, seed):\n",
        "    \n",
        "    # first, build the feedforward net\n",
        "    xs, (ws, bs), zs, ys = vanilla_net(input_dim, hidden_units, hidden_layers, seed)\n",
        "    \n",
        "    # then, build its differentiation by backprop\n",
        "    xbar = backprop((ws, bs), zs)\n",
        "    \n",
        "    # return input x, output y and differentials d_y/d_z\n",
        "    return xs, ys, xbar"
      ],
      "metadata": {
        "id": "CWUo4p_OtsDa"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Training***"
      ],
      "metadata": {
        "id": "M9bWbSO4BNfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def diff_training_graph(\n",
        "    # same as vanilla\n",
        "    input_dim, \n",
        "    hidden_units, \n",
        "    hidden_layers, \n",
        "    seed, \n",
        "    # balance relative weight of values and differentials \n",
        "    # loss = alpha * MSE(values) + beta * MSE(greeks, lambda_j) \n",
        "    # see online appendix\n",
        "    alpha, \n",
        "    beta,\n",
        "    lambda_j):\n",
        "    \n",
        "    # net, now a twin\n",
        "    inputs, predictions, derivs_predictions = twin_net(input_dim, hidden_units, hidden_layers, seed)\n",
        "    \n",
        "    # placeholder for labels, now also derivs labels\n",
        "    labels = tf.compat.v1.placeholder(shape=[None, 1], dtype=real_type)\n",
        "    derivs_labels = tf.compat.v1.placeholder(shape=[None, derivs_predictions.shape[2]], dtype=real_type) ### changed shape here\n",
        "    \n",
        "    # loss, now combined values + derivatives\n",
        "    loss = alpha * tf.losses.mean_squared_error(labels, predictions) \\\n",
        "    + beta * tf. losses.mean_squared_error(derivs_labels * lambda_j, derivs_predictions * lambda_j)\n",
        "    \n",
        "    # optimizer, as vanilla\n",
        "    learning_rate = tf.compat.v1.placeholder(real_type)\n",
        "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "    \n",
        "    # return all necessary tensors, including derivatives\n",
        "    # predictions and labels\n",
        "    return inputs, labels, derivs_labels, predictions, derivs_predictions, \\\n",
        "            learning_rate, loss, optimizer.minimize(loss)\n",
        "\n",
        "def diff_train_one_epoch(inputs, labels, derivs_labels, \n",
        "                         # graph\n",
        "                         lr_placeholder, minimizer,             \n",
        "                         # training set, extended\n",
        "                         x_train, y_train, dydx_train,          \n",
        "                         # params\n",
        "                         learning_rate, batch_size, session):   \n",
        "    \n",
        "    m, n = x_train.shape\n",
        "    \n",
        "    # minimization loop, now with Greeks\n",
        "    first = 0\n",
        "    last = min(batch_size, m)\n",
        "    while first < m:\n",
        "        session.run(minimizer, feed_dict = {\n",
        "            inputs: x_train[first:last], \n",
        "            labels: y_train[first:last], ### here is where we set the targets\n",
        "            derivs_labels: dydx_train[first:last], ### here is where we set the targets\n",
        "            lr_placeholder: learning_rate\n",
        "        })\n",
        "        first = last\n",
        "        last = min(first + batch_size, m)"
      ],
      "metadata": {
        "id": "_NmkE6yqBM51"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(description,\n",
        "          # neural approximator\n",
        "          approximator,              \n",
        "          # training params\n",
        "          reinit=True, \n",
        "          epochs=100, \n",
        "          # one-cycle learning rate schedule\n",
        "          learning_rate_schedule=[    (0.0, 1.0e-8), \\\n",
        "                                      (0.2, 0.1),    \\\n",
        "                                      (0.6, 0.01),   \\\n",
        "                                      (0.9, 1.0e-6), \\\n",
        "                                      (1.0, 1.0e-8)  ], \n",
        "          batches_per_epoch=16,\n",
        "          min_batch_size=256,\n",
        "          # callback function and when to call it\n",
        "          callback=None,           # arbitrary callable\n",
        "          callback_epochs=[]):     # call after what epochs, e.g. [5, 20]\n",
        "              \n",
        "    # batching\n",
        "    batch_size = max(min_batch_size, approximator.m // batches_per_epoch)\n",
        "    \n",
        "    # one-cycle learning rate sechedule\n",
        "    lr_schedule_epochs, lr_schedule_rates = zip(*learning_rate_schedule)\n",
        "            \n",
        "    # reset\n",
        "    if reinit:\n",
        "        approximator.session.run(approximator.initializer)\n",
        "    \n",
        "    # callback on epoch 0, if requested\n",
        "    if callback and 0 in callback_epochs:\n",
        "        callback(approximator, 0)\n",
        "        \n",
        "    # loop on epochs, with progress bar (tqdm)\n",
        "    for epoch in tqdm_notebook(range(epochs), desc=description):\n",
        "        \n",
        "        # interpolate learning rate in cycle\n",
        "        learning_rate = np.interp(epoch / epochs, lr_schedule_epochs, lr_schedule_rates)\n",
        "        \n",
        "        # train one epoch\n",
        "        diff_train_one_epoch(\n",
        "        approximator.inputs, \n",
        "        approximator.labels, \n",
        "        approximator.derivs_labels,\n",
        "        approximator.learning_rate, \n",
        "        approximator.minimizer, \n",
        "        approximator.x, \n",
        "        approximator.y, \n",
        "        approximator.dy_dx,\n",
        "        learning_rate, \n",
        "        batch_size, \n",
        "        approximator.session)\n",
        "        \n",
        "        # callback, if requested\n",
        "        if callback and epoch in callback_epochs:\n",
        "            callback(approximator, epoch)\n",
        "\n",
        "    # final callback, if requested\n",
        "    if callback and epochs in callback_epochs:\n",
        "        callback(approximator, epochs)        \n"
      ],
      "metadata": {
        "id": "xu60P9HdEC4-"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Normalise data***"
      ],
      "metadata": {
        "id": "YDMZ2nOUSNEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# basic data preparation\n",
        "epsilon = 1.0e-08\n",
        "def normalize_data(x_raw, y_raw, dydx_raw = None):\n",
        "    \n",
        "    # normalize dataset\n",
        "    x_mean = x_raw.mean(axis=0)\n",
        "    x_std = x_raw.std(axis=0) + epsilon\n",
        "    x = (x_raw - x_mean) / x_std\n",
        "    y_mean = y_raw.mean(axis=0)\n",
        "    y_std = y_raw.std(axis=0) + epsilon\n",
        "    y = (y_raw-y_mean) / y_std   \n",
        "    # normalize derivatives\n",
        "    dy_dx = dydx_raw / y_std * x_std \n",
        "    # weights of derivatives in cost function = (quad) mean size\n",
        "    lambda_j = 1.0 / np.sqrt((dy_dx ** 2).mean(axis=0)).reshape(1, -1)\n",
        "   \n",
        "    return x_mean, x_std, x, y_mean, y_std, y, dy_dx, lambda_j"
      ],
      "metadata": {
        "id": "2AK3Y8ZIEnRM"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neural_Approximator():\n",
        "    \n",
        "    def __init__(self, x_raw, y_raw, \n",
        "                 dydx_raw=None):      # derivatives labels, \n",
        "       \n",
        "        self.x_raw = x_raw\n",
        "        self.y_raw = y_raw\n",
        "        self.dydx_raw = dydx_raw\n",
        "\n",
        "    def build_graph(self,         \n",
        "                differential,       # differential or not           \n",
        "                lam,                # balance cost between values and derivs  \n",
        "                hidden_units, \n",
        "                hidden_layers, \n",
        "                weight_seed):\n",
        "  \n",
        "        self.graph = tf.Graph()     # Graphs are used to represent the function's computations.  \n",
        "        \n",
        "        with self.graph.as_default():\n",
        "        \n",
        "            # build the graph (add parameters)\n",
        "            self.differential = differential          \n",
        "            self.alpha = 1.0 / (1.0 + lam * self.n)\n",
        "            self.beta = 1.0 - self.alpha             \n",
        "            self.inputs, \\\n",
        "            self.labels, \\\n",
        "            self.derivs_labels, \\\n",
        "            self.predictions, \\\n",
        "            self.derivs_predictions, \\\n",
        "            self.learning_rate, \\\n",
        "            self.loss, \\\n",
        "            self.minimizer = diff_training_graph(self.n, hidden_units, \\\n",
        "                                                     hidden_layers, weight_seed, \\\n",
        "                                                     self.alpha, self.beta, self.lambda_j)\n",
        "        \n",
        "            # global initializer\n",
        "            self.initializer = tf.compat.v1.global_variables_initializer()\n",
        "            \n",
        "        # done\n",
        "        self.graph.finalize()\n",
        "        self.session = tf.compat.v1.Session(graph=self.graph)\n",
        "                        \n",
        "    # prepare for training with m examples, standard or differential\n",
        "    def prepare(self, \n",
        "                m, \n",
        "                differential,\n",
        "                lam=1,              # balance cost between values and derivs  \n",
        "                # standard architecture\n",
        "                hidden_units=20, \n",
        "                hidden_layers=4, \n",
        "                weight_seed=None):\n",
        "\n",
        "        # prepare dataset (normalise)\n",
        "        self.x_mean, self.x_std, self.x, self.y_mean, self.y_std, self.y, self.dy_dx, self.lambda_j = \\\n",
        "            normalize_data(self.x_raw, self.y_raw, self.dydx_raw)\n",
        "        \n",
        "        # build graph        \n",
        "        self.m, self.n = self.x.shape        \n",
        "        self.build_graph(differential, lam, hidden_units, hidden_layers, weight_seed)\n",
        "        \n",
        "    def train(self,            \n",
        "              description=\"training\",\n",
        "              # training params\n",
        "              reinit=True, \n",
        "              epochs=100, \n",
        "              # one-cycle learning rate schedule\n",
        "              learning_rate_schedule=[\n",
        "                  (0.0, 1.0e-8), \n",
        "                  (0.2, 0.1), \n",
        "                  (0.6, 0.01), \n",
        "                  (0.9, 1.0e-6), \n",
        "                  (1.0, 1.0e-8)], \n",
        "              batches_per_epoch=16,\n",
        "              min_batch_size=256,\n",
        "              # callback and when to call it\n",
        "              # we don't use callbacks, but this is very useful, e.g. for debugging\n",
        "              callback=None,           # arbitrary callable\n",
        "              callback_epochs=[]):     # call after what epochs, e.g. [5, 20]\n",
        "              \n",
        "        train(description, \n",
        "              self, \n",
        "              reinit, \n",
        "              epochs, \n",
        "              learning_rate_schedule, \n",
        "              batches_per_epoch, \n",
        "              min_batch_size,\n",
        "              callback, \n",
        "              callback_epochs)\n",
        "     \n",
        "    def predict_values(self, x):\n",
        "        # scale\n",
        "        x_scaled = (x-self.x_mean) / self.x_std \n",
        "        # predict scaled\n",
        "        y_scaled = self.session.run(self.predictions, feed_dict = {self.inputs: x_scaled})\n",
        "        # unscale\n",
        "        y = self.y_mean + self.y_std * y_scaled\n",
        "        return y\n",
        "\n",
        "    def predict_values_and_derivs(self, x):\n",
        "        # scale\n",
        "        x_scaled = (x-self.x_mean) / self.x_std\n",
        "        # predict scaled\n",
        "        y_scaled, dyscaled_dxscaled = self.session.run(\n",
        "            [self.predictions, self.derivs_predictions], \n",
        "            feed_dict = {self.inputs: x_scaled})\n",
        "        # unscale\n",
        "        y = self.y_mean + self.y_std * y_scaled\n",
        "        dydx = self.y_std / self.x_std * dyscaled_dxscaled\n",
        "        return y, dydx"
      ],
      "metadata": {
        "id": "-i0FcLJfEpVR"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implementation***"
      ],
      "metadata": {
        "id": "hKGm6k3MFK1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(generator,         # how we want to generate the data (Black Scholes)\n",
        "         sizes,             # size of the data\n",
        "         nTest,             # number of test sets\n",
        "         simulSeed=None,    # seed for training data generation\n",
        "         testSeed=None,     # seed for test data generation\n",
        "         weightSeed=None,   # seed for weight initialisation\n",
        "         deltidx=0):        # ???\n",
        "\n",
        "    ### simulation\n",
        "    print(\"simulating training, valid and test sets\")\n",
        "    # generate train set\n",
        "    xTrain, yTrain, dydxTrain = generator.trainingSet(max(sizes), seed=simulSeed)\n",
        "    # generate test sets\n",
        "    xTest, xAxis, yTest, dydxTest, vegas = generator.testSet(num=nTest, seed=testSeed) # vega is not being used???\n",
        "    print(\"done\")\n",
        "\n",
        "    # neural approximator\n",
        "    print(\"initializing neural appropximator\")\n",
        "    regressor = Neural_Approximator(xTrain, yTrain, dydxTrain) ### what are the targets?\n",
        "    print(\"done\")\n",
        "    \n",
        "    predvalues = {}    \n",
        "    preddeltas = {}\n",
        "    for size in sizes:        \n",
        "            \n",
        "        print(\"\\nsize %d\" % size)\n",
        "        regressor.prepare(size, False, weight_seed=weightSeed)\n",
        "            \n",
        "        t0 = time.time()\n",
        "        regressor.train(\"standard training\")\n",
        "        predictions, deltas = regressor.predict_values_and_derivs(xTest)\n",
        "        predvalues[(\"standard\", size)] = predictions\n",
        "        preddeltas[(\"standard\", size)] = deltas[:, deltidx]\n",
        "        t1 = time.time()\n",
        "        \n",
        "        regressor.prepare(size, True, weight_seed=weightSeed)\n",
        "            \n",
        "        t0 = time.time()\n",
        "        regressor.train(\"differential training\")\n",
        "        predictions, deltas = regressor.predict_values_and_derivs(xTest)\n",
        "        predvalues[(\"differential\", size)] = predictions\n",
        "        preddeltas[(\"differential\", size)] = deltas[:, deltidx]\n",
        "        t1 = time.time()\n",
        "        \n",
        "    return xAxis, yTest, dydxTest[:, deltidx], vegas, predvalues, preddeltas\n"
      ],
      "metadata": {
        "id": "Ocr8tAUjI1Nw"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = BlackScholes()\n",
        "print(xTest.shape, xAxis.shape, yTest.shape, vegas.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3vyikMY3s0j",
        "outputId": "4e5c3e46-4803-4bc3-a8fb-8dd305a36d6c"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 1) (100, 1) (100, 1) (100, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def graph(title, \n",
        "          predictions, \n",
        "          xAxis, \n",
        "          xAxisName, \n",
        "          yAxisName, \n",
        "          targets, \n",
        "          sizes, \n",
        "          computeRmse=False, \n",
        "          weights=None):\n",
        "    \n",
        "    numRows = len(sizes)\n",
        "    numCols = 2\n",
        "\n",
        "    fig, ax = plt.subplots(numRows, numCols, squeeze=False)\n",
        "    fig.set_size_inches(4 * numCols + 1.5, 4 * numRows)\n",
        "\n",
        "    for i, size in enumerate(sizes):\n",
        "        ax[i,0].annotate(\"size %d\" % size, xy=(0, 0.5), \n",
        "          xytext=(-ax[i,0].yaxis.labelpad-5, 0),\n",
        "          xycoords=ax[i,0].yaxis.label, textcoords='offset points',\n",
        "          ha='right', va='center')\n",
        "  \n",
        "    ax[0,0].set_title(\"standard\")\n",
        "    ax[0,1].set_title(\"differential\")\n",
        "    \n",
        "    for i, size in enumerate(sizes):        \n",
        "        for j, regType, in enumerate([\"standard\", \"differential\"]):\n",
        "\n",
        "            if computeRmse:\n",
        "                errors = 100 * (predictions[(regType, size)] - targets)\n",
        "                if weights is not None:\n",
        "                    errors /= weights\n",
        "                rmse = np.sqrt((errors ** 2).mean(axis=0))\n",
        "                t = \"rmse %.2f\" % rmse\n",
        "            else:\n",
        "                t = xAxisName\n",
        "                \n",
        "            ax[i,j].set_xlabel(t)            \n",
        "            ax[i,j].set_ylabel(yAxisName)\n",
        "\n",
        "            ax[i,j].plot(xAxis*100, predictions[(regType, size)]*100, 'co', \\\n",
        "                         markersize=2, markerfacecolor='white', label=\"predicted\")\n",
        "            ax[i,j].plot(xAxis*100, targets*100, 'r.', markersize=0.5, label='targets')\n",
        "\n",
        "            ax[i,j].legend(prop={'size': 8}, loc='upper left')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.9)\n",
        "    plt.suptitle(\"% s -- %s\" % (title, yAxisName), fontsize=16)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KcCi7yQ127KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simulation set sizes to perform\n",
        "sizes = [1000, 1000]\n",
        "\n",
        "# show delta?\n",
        "showDeltas = True\n",
        "\n",
        "# seed\n",
        "# simulSeed = 1234\n",
        "simulSeed = np.random.randint(0, 10000) \n",
        "print(\"using seed %d\" % simulSeed)\n",
        "weightSeed = None\n",
        "\n",
        "# number of test scenarios\n",
        "nTest = 100    \n",
        "\n",
        "# go\n",
        "generator = BlackScholes()\n",
        "xAxis, yTest, dydxTest, vegas, values, deltas = \\\n",
        "    test(generator, sizes, nTest, simulSeed, None, weightSeed)"
      ],
      "metadata": {
        "id": "kdCJRWXBGvnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VB-9Z1_OQvvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show predicitions\n",
        "graph(\"Black & Scholes\", values, xAxis, \"\", \"values\", yTest, sizes, True)\n",
        "\n",
        "# show deltas\n",
        "graph(\"Black & Scholes\", deltas, xAxis, \"\", \"deltas\", dydxTest, sizes, True)"
      ],
      "metadata": {
        "id": "4frfxa83OFMX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}